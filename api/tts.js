/**
 * API endpoint para generar audio usando Vertex AI Text-to-Speech
 * Usa la voz Algieba con el modelo gemini-2.5-pro-tts
 * REQUIERE: Vertex AI API habilitada
 */

const crypto = require('crypto');
const { GoogleAuth } = require('google-auth-library');
require('dotenv').config();

// Configuraci√≥n de Vertex AI
const PROJECT_ID = process.env.VERTEX_AI_PROJECT_ID || 'cronosai-473114';
const LOCATION = process.env.VERTEX_AI_LOCATION || 'us-central1';

// Mapeo de idiomas
const languageCodes = {
  es: 'es-es',
  en: 'en-us',
  de: 'de-de',
  it: 'it-it',
  fr: 'fr-fr',
  pt: 'pt-br'
};

// Configuraci√≥n de la voz Algieba
const VOICE_NAME = 'Algieba';
const MODEL_NAME = 'gemini-2.5-flash-tts'; // OPTIMIZACI√ìN: Usar Flash en lugar de Pro para mayor velocidad en llamadas telef√≥nicas

// OPTIMIZACI√ìN CR√çTICA: Limitar texto para reducir latencia de TTS
// La latencia crece casi linealmente con la longitud del texto
const MAX_TEXT_LENGTH = 180; // Caracteres m√°ximos para llamadas telef√≥nicas (1-2 frases cortas)

/**
 * Prepara el texto para llamadas telef√≥nicas limitando su longitud
 * Esto reduce significativamente la latencia de TTS (de varios segundos a <1s)
 */
function prepareTextForCall(rawText) {
  if (!rawText || typeof rawText !== 'string') {
    return '';
  }
  
  let text = rawText.trim();
  
  // Quitar saltos de l√≠nea y espacios m√∫ltiples
  text = text.replace(/\s+/g, ' ');
  
  // Si el texto es muy corto, devolverlo tal cual
  if (text.length <= MAX_TEXT_LENGTH) {
    return text;
  }
  
  // Intentar cortar en el punto m√°s cercano a MAX_TEXT_LENGTH
  const cutPoint = text.lastIndexOf('.', MAX_TEXT_LENGTH);
  if (cutPoint > 50) {
    // Si encontramos un punto razonablemente cerca, cortar ah√≠
    text = text.slice(0, cutPoint + 1);
  } else {
    // Si no hay punto cercano, cortar y agregar elipsis
    text = text.slice(0, MAX_TEXT_LENGTH) + '‚Ä¶';
  }
  
  return text;
}

// Cache optimizado para mejor rendimiento
const audioCache = new Map();
const CACHE_TTL_MS = 60 * 60 * 1000; // 1 hora
const MAX_CACHE_SIZE = 500; // OPTIMIZACI√ìN: Reducido de 1000 a 500 para mejor gesti√≥n de memoria
const MAX_CACHE_MEMORY_MB = 50; // OPTIMIZACI√ìN: L√≠mite de memoria en MB (aprox 50MB de audio en cache)

/**
 * Limpia el cache eliminando entradas expiradas y las m√°s antiguas si excede l√≠mites
 */
function cleanupCache() {
  const now = Date.now();
  let totalMemoryBytes = 0;
  const entries = [];
  
  // Calcular memoria total y recopilar entradas v√°lidas
  for (const [hash, cached] of audioCache.entries()) {
    const age = now - cached.timestamp;
    if (age < CACHE_TTL_MS) {
      // Entrada v√°lida
      const size = cached.audio ? cached.audio.length : 0;
      totalMemoryBytes += size;
      entries.push({ hash, timestamp: cached.timestamp, size });
    } else {
      // Entrada expirada - eliminar
      audioCache.delete(hash);
    }
  }
  
  // Si excede l√≠mite de memoria, eliminar las m√°s antiguas
  const maxMemoryBytes = MAX_CACHE_MEMORY_MB * 1024 * 1024;
  if (totalMemoryBytes > maxMemoryBytes) {
    // Ordenar por timestamp (m√°s antiguas primero)
    entries.sort((a, b) => a.timestamp - b.timestamp);
    
    // Eliminar hasta que estemos bajo el l√≠mite
    for (const entry of entries) {
      if (totalMemoryBytes <= maxMemoryBytes) break;
      audioCache.delete(entry.hash);
      totalMemoryBytes -= entry.size;
    }
    
    console.log(`üßπ [TTS] Cache limpiado por memoria. Eliminadas entradas antiguas. Memoria actual: ${(totalMemoryBytes / 1024 / 1024).toFixed(2)}MB`);
  }
  
  // Si excede l√≠mite de entradas, eliminar las m√°s antiguas
  if (audioCache.size > MAX_CACHE_SIZE) {
    entries.sort((a, b) => a.timestamp - b.timestamp);
    const toDelete = audioCache.size - MAX_CACHE_SIZE;
    for (let i = 0; i < toDelete; i++) {
      audioCache.delete(entries[i].hash);
    }
    console.log(`üßπ [TTS] Cache limpiado por tama√±o. Eliminadas ${toDelete} entradas antiguas. Tama√±o actual: ${audioCache.size}`);
  }
}

// OPTIMIZACI√ìN: Pre-generar respuestas comunes para reducir latencia
let commonResponsesPreGenerated = false;
const commonResponsesToPreGenerate = [
  // Mensajes de greeting en todos los idiomas
  { text: '¬°Buenos d√≠as! Qu√© gusto tenerle por aqu√≠. ¬øC√≥mo puedo ayudarle?', language: 'es' },
  { text: '¬°Buenas tardes! Encantado de atenderle. ¬øEn qu√© puedo ayudarle?', language: 'es' },
  { text: '¬°Buenas noches! Bienvenido. ¬øC√≥mo puedo ayudarle?', language: 'es' },
  { text: 'Good morning! How can I help you?', language: 'en' },
  { text: 'Good afternoon! How can I assist you?', language: 'en' },
  { text: 'Good evening! How can I help you?', language: 'en' },
  // Mensajes de ask_people
  { text: '¬øPara cu√°ntas personas ser√° la reserva?', language: 'es' },
  { text: '¬øCu√°ntas personas ser√°n?', language: 'es' },
  { text: 'How many people will the reservation be for?', language: 'en' },
  { text: 'How many people?', language: 'en' },
  // Mensajes de ask_date
  { text: '¬øPara qu√© fecha desea la reserva?', language: 'es' },
  { text: '¬øQu√© d√≠a prefiere?', language: 'es' },
  { text: 'What date would you like the reservation for?', language: 'en' },
  { text: 'What day do you prefer?', language: 'en' },
  // Mensajes de ask_time
  { text: '¬øA qu√© hora desea la reserva?', language: 'es' },
  { text: '¬øQu√© hora prefiere?', language: 'es' },
  { text: 'What time would you like the reservation?', language: 'en' },
  { text: 'What time do you prefer?', language: 'en' },
  // Mensajes de ask_name
  { text: '¬øA nombre de qui√©n ser√° la reserva?', language: 'es' },
  { text: '¬øMe puede decir su nombre?', language: 'es' },
  { text: 'What name should the reservation be under?', language: 'en' },
  { text: 'Can you tell me your name?', language: 'en' },
  // Mensajes de confirmaci√≥n
  { text: 'Perfecto, ¬øest√° todo correcto?', language: 'es' },
  { text: 'Perfect, is everything correct?', language: 'en' },
  // Mensajes de error comunes
  { text: 'Disculpe, no he entendido bien. ¬øPodr√≠a repetir, por favor?', language: 'es' },
  { text: 'Sorry, I didn\'t understand. Could you repeat, please?', language: 'en' }
];

/**
 * Pre-genera respuestas comunes en background para reducir latencia
 */
async function preGenerateCommonResponses() {
  if (commonResponsesPreGenerated) {
    return;
  }
  
  console.log(`üé§ [TTS] Pre-generando ${commonResponsesToPreGenerate.length} respuestas comunes...`);
  const startTime = Date.now();
  
  // Generar en paralelo (m√°ximo 5 a la vez para no sobrecargar)
  const batchSize = 5;
  for (let i = 0; i < commonResponsesToPreGenerate.length; i += batchSize) {
    const batch = commonResponsesToPreGenerate.slice(i, i + batchSize);
    await Promise.all(
      batch.map(async ({ text, language }) => {
        try {
          const hash = crypto.createHash('md5').update(`${text}-${languageCodes[language] || languageCodes.es}`).digest('hex');
          // Solo generar si no est√° en cache
          if (!audioCache.has(hash)) {
            await generateAudioWithVertexAI(text, language).catch(err => {
              console.warn(`‚ö†Ô∏è [TTS] Error pre-generando "${text.substring(0, 30)}...": ${err.message}`);
            });
          }
        } catch (error) {
          // Ignorar errores en pre-generaci√≥n (no cr√≠tico)
          console.warn(`‚ö†Ô∏è [TTS] Error pre-generando respuesta com√∫n: ${error.message}`);
        }
      })
    );
  }
  
  const preGenTime = Date.now() - startTime;
  console.log(`‚úÖ [TTS] Pre-generaci√≥n completada en ${preGenTime}ms. ${audioCache.size} audios en cache.`);
  commonResponsesPreGenerated = true;
}

// DESACTIVADO: Pre-generaci√≥n causa error 429 (quota exceeded)
// Iniciar pre-generaci√≥n en background (no bloquea)
// setImmediate(() => {
//   preGenerateCommonResponses().catch(err => {
//     console.warn(`‚ö†Ô∏è [TTS] Error en pre-generaci√≥n inicial: ${err.message}`);
//   });
// });

// Cliente de autenticaci√≥n
let authClient = null;
let cachedAccessToken = null;
let tokenExpiryTime = 0;
const TOKEN_CACHE_DURATION_MS = 50 * 60 * 1000; // Cachear token por 50 minutos (los tokens duran ~1 hora)

async function getAccessToken() {
  // Verificar si tenemos un token v√°lido en cache
  if (cachedAccessToken && Date.now() < tokenExpiryTime) {
    return cachedAccessToken;
  }

  if (!authClient) {
    try {
      const credentialsJson = process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON;
      
      if (!credentialsJson) {
        throw new Error('‚ùå GOOGLE_APPLICATION_CREDENTIALS_JSON no est√° configurada.');
      }

      const credentials = typeof credentialsJson === 'string' 
        ? JSON.parse(credentialsJson) 
        : credentialsJson;

      authClient = new GoogleAuth({
        credentials: credentials,
        scopes: ['https://www.googleapis.com/auth/cloud-platform']
      });
      
      console.log(`‚úÖ [TTS] Cliente de autenticaci√≥n inicializado: ${credentials.client_email}`);
    } catch (error) {
      console.error('‚ùå [TTS] Error inicializando cliente:', error);
      throw error;
    }
  }
  
  const client = await authClient.getClient();
  const accessTokenResponse = await client.getAccessToken();
  
  if (!accessTokenResponse.token) {
    throw new Error('‚ùå No se pudo obtener el token de acceso');
  }
  
  // Cachear el token
  cachedAccessToken = accessTokenResponse.token;
  tokenExpiryTime = Date.now() + TOKEN_CACHE_DURATION_MS;
  
  return cachedAccessToken;
}

async function generateAudioWithVertexAI(text, language = 'es') {
  // OPTIMIZACI√ìN CR√çTICA: Limitar texto antes de procesar (reduce latencia de 4-8s a <1s)
  const preparedText = prepareTextForCall(text);
  const originalLength = text.length;
  const preparedLength = preparedText.length;
  
  if (originalLength > preparedLength) {
    console.log(`‚úÇÔ∏è [TTS] Texto recortado de ${originalLength} a ${preparedLength} caracteres para reducir latencia`);
  }
  
  const languageCode = languageCodes[language] || languageCodes.es;
  const hash = crypto.createHash('md5').update(`${preparedText}-${languageCode}`).digest('hex');
  
  // Verificar cache PRIMERO (m√°s r√°pido)
  const cached = audioCache.get(hash);
  if (cached) {
    const age = Date.now() - cached.timestamp;
    if (age < CACHE_TTL_MS) {
      console.log(`‚úÖ [TTS] Cache hit para hash: ${hash.substring(0, 8)}... (${cached.audio.length} bytes, edad: ${Math.round(age / 1000)}s)`);
      return { audio: cached.audio, hash };
    } else {
      // Entrada expirada - eliminar
      audioCache.delete(hash);
    }
  }
  
  // OPTIMIZACI√ìN: Si no est√° en cache y es una respuesta com√∫n, intentar pre-generarla
  // (esto se hace autom√°ticamente en background, pero aqu√≠ verificamos si ya est√°)

  try {
    const accessToken = await getAccessToken();
    const ttsGenerationStartTime = Date.now();
    
    console.log(`üé§ [TTS] Generando audio con Vertex AI: "${preparedText.substring(0, 50)}..." (${languageCode}) - INICIO`);

    // Endpoint est√°ndar de Text-to-Speech API (el modelo gemini-2.5-pro-tts se especifica en el request body)
    const url = 'https://texttospeech.googleapis.com/v1beta1/text:synthesize';

    const requestBody = {
      audioConfig: {
        audioEncoding: 'MULAW', // OPTIMIZACI√ìN CR√çTICA: MULAW es ideal para telefon√≠a (formato nativo de Twilio, menos bytes)
        pitch: 0,
        speakingRate: 1.0, // Velocidad normal de habla (1.0 = velocidad est√°ndar)
        sampleRateHertz: 8000, // OPTIMIZACI√ìN CR√çTICA: 8000 Hz (mismo sample rate que Twilio, suficiente para voz telef√≥nica)
        volumeGainDb: 0 // Sin ganancia adicional para mantener velocidad
      },
      input: {
        text: preparedText // Usar texto preparado (limitado)
      },
      voice: {
        languageCode: languageCode,
        name: VOICE_NAME,
        modelName: MODEL_NAME // gemini-2.5-pro-tts (requiere Vertex AI)
      }
    };

    console.log(`üîç [TTS] Text-to-Speech API Request:`, {
      projectId: PROJECT_ID,
      location: LOCATION,
      languageCode: languageCode,
      voiceName: VOICE_NAME,
      modelName: MODEL_NAME,
      url: url
    });

    // OPTIMIZACI√ìN: Timeout de 10 segundos para dar tiempo a la generaci√≥n de audio
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 10000);
    
    let response;
    try {
      response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${accessToken}`
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal
      });
      clearTimeout(timeoutId);
    } catch (fetchError) {
      clearTimeout(timeoutId);
      if (fetchError.name === 'AbortError') {
        throw new Error('TTS fetch timeout after 10s');
      }
      throw fetchError;
    }

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`‚ùå [TTS] Error en Vertex AI: ${response.status} - ${errorText}`);
      
      // Mensajes de error m√°s descriptivos
      let errorMessage = `Error en Vertex AI TTS: ${response.status}`;
      
      if (response.status === 403) {
        errorMessage = `‚ùå Permisos denegados. Verifica que:
1. El Service Account tiene el rol "Vertex AI User" (roles/aiplatform.user)
2. La API "Vertex AI API" est√° habilitada
3. Las credenciales JSON son correctas
4. El proyecto ID es correcto: ${PROJECT_ID}
Error: ${errorText}`;
      } else if (response.status === 401) {
        errorMessage = `‚ùå Autenticaci√≥n fallida. Verifica que:
1. Las credenciales JSON son correctas
2. El Service Account existe y est√° activo
3. Las credenciales no han expirado
Error: ${errorText}`;
      } else if (response.status === 429) {
        // ERROR 429: Quota exceeded - fallback inmediato sin retry
        errorMessage = `TTS quota exceeded (429) - using fallback`;
        const quotaError = new Error(errorMessage);
        quotaError.statusCode = 429;
        quotaError.isQuotaError = true;
        throw quotaError;
      } else if (response.status === 400) {
        errorMessage = `‚ùå Solicitud inv√°lida. Verifica que:
1. El c√≥digo de idioma es correcto (${languageCode})
2. La voz "Algieba" est√° disponible para el idioma ${languageCode}
3. El modelo "${MODEL_NAME}" es v√°lido y est√° disponible
4. La API "Cloud Text-to-Speech API" est√° habilitada
5. El modelo "${MODEL_NAME}" requiere Vertex AI API habilitada
Error: ${errorText}`;
      } else if (response.status === 404) {
        errorMessage = `‚ùå Endpoint no encontrado. Verifica que:
1. La API "Cloud Text-to-Speech API" est√° habilitada
2. El endpoint es correcto
3. El modelo "${MODEL_NAME}" est√° disponible
Error: ${errorText}`;
      } else {
        errorMessage = `Error en Text-to-Speech API: ${response.status} - ${errorText}`;
      }
      
      throw new Error(errorMessage);
    }

    const data = await response.json();
    
    if (!data.audioContent) {
      throw new Error('No se recibi√≥ audioContent en la respuesta');
    }

    const audioBuffer = Buffer.from(data.audioContent, 'base64');

    // OPTIMIZACI√ìN: Limpiar cache antes de agregar nueva entrada (evita acumulaci√≥n de memoria)
    cleanupCache();

    // Guardar en cache
    audioCache.set(hash, {
      audio: audioBuffer,
      timestamp: Date.now(),
      language: languageCode,
      text: preparedText.substring(0, 100)
    });

    const ttsGenerationTime = Date.now() - ttsGenerationStartTime;
    console.log(`‚úÖ [TTS] Audio generado exitosamente en ${ttsGenerationTime}ms (${audioBuffer.length} bytes)`);

    return { audio: audioBuffer, hash };
  } catch (error) {
    console.error('‚ùå [TTS] Error generando audio con Vertex AI:', error);
    throw error;
  }
}

module.exports = async function handler(req, res) {
  const { method, query, body } = req;
  
  if (method === 'GET') {
    try {
      const { hash, text, language = 'es' } = query;
      
      if (!hash && !text) {
        return res.status(400).json({ error: 'Hash or text is required' });
      }
      
      let audioData;
      
      if (hash) {
        // OPTIMIZACI√ìN: Buscar por hash primero (m√°s r√°pido)
        const cached = audioCache.get(hash);
        if (cached && (Date.now() - cached.timestamp) < CACHE_TTL_MS) {
          console.log(`‚úÖ [TTS] Cache hit por hash: ${hash.substring(0, 8)}... (${cached.audio.length} bytes)`);
          audioData = { audio: cached.audio, hash };
        } else {
          return res.status(404).json({ error: 'Audio not found in cache' });
        }
      } else if (text) {
        const decodedText = decodeURIComponent(text);
        const textHash = crypto.createHash('md5').update(`${decodedText}-${languageCodes[language] || languageCodes.es}`).digest('hex');
        
        // OPTIMIZACI√ìN: Verificar cache antes de generar
        const cached = audioCache.get(textHash);
        if (cached && (Date.now() - cached.timestamp) < CACHE_TTL_MS) {
          console.log(`‚úÖ [TTS] Cache hit por texto: "${decodedText.substring(0, 30)}..." (${cached.audio.length} bytes)`);
          audioData = { audio: cached.audio, hash: textHash };
        } else {
          // OPTIMIZACI√ìN CR√çTICA: Generar en background y responder inmediatamente si es texto corto
          // Para textos largos, generar normalmente
          if (decodedText.length > 200) {
            // Texto largo: generar normalmente
            console.log(`üé§ [TTS] Generando audio para texto largo (${decodedText.length} chars)...`);
            audioData = await generateAudioWithVertexAI(decodedText, language);
          } else {
            // Texto corto: intentar generar r√°pido con timeout m√°s largo
            console.log(`üé§ [TTS] Generando audio r√°pido para: "${decodedText.substring(0, 50)}..."`);
            try {
              // OPTIMIZACI√ìN: Timeout de 10 segundos para dar tiempo a la generaci√≥n de audio
              const ttsTimeout = 10000; // 10 segundos m√°ximo
              audioData = await Promise.race([
                generateAudioWithVertexAI(decodedText, language),
                new Promise((_, reject) => 
                  setTimeout(() => reject(new Error(`TTS timeout after ${ttsTimeout}ms`)), ttsTimeout)
                )
              ]);
            } catch (error) {
              console.error(`‚ùå [TTS] Error o timeout generando audio: ${error.message}`);
              // OPTIMIZACI√ìN: En lugar de fallar, intentar usar Twilio Say como fallback
              // Pero como estamos en el endpoint TTS, mejor devolver error y que Twilio use Say
              return res.status(500).json({ 
                error: 'Failed to generate audio',
                message: error.message,
                fallback: 'Use Twilio Say instead'
              });
            }
          }
        }
      }
      
      if (!audioData || !audioData.audio) {
        return res.status(500).json({ error: 'Failed to generate audio' });
      }
      
      // OPTIMIZACI√ìN: MULAW usa Content-Type audio/basic (formato telef√≥nico nativo)
      res.setHeader('Content-Type', 'audio/basic');
      res.setHeader('Content-Length', audioData.audio.length);
      res.setHeader('Cache-Control', 'public, max-age=3600');
      res.setHeader('X-Audio-Hash', audioData.hash);
      res.setHeader('X-Audio-Language', language);
      res.setHeader('X-Voice-Name', VOICE_NAME);
      res.setHeader('X-Model-Name', MODEL_NAME);
      res.setHeader('X-Vertex-AI', 'true');
      
      return res.status(200).send(audioData.audio);
    } catch (error) {
      console.error('‚ùå [TTS] Error en GET endpoint:', error);
      return res.status(500).json({ 
        error: 'Error retrieving audio',
        message: error.message 
      });
    }
  }
  
  if (method === 'POST') {
    try {
      const { text, language = 'es' } = body;

      if (!text || typeof text !== 'string') {
        return res.status(400).json({ error: 'Text is required' });
      }

      if (text.length > 5000) {
        return res.status(400).json({ error: 'Text too long (max 5000 characters)' });
      }

      // OPTIMIZACI√ìN: Limitar texto antes de generar audio
      const preparedText = prepareTextForCall(text);
      const audioData = await generateAudioWithVertexAI(preparedText, language);

      // OPTIMIZACI√ìN: MULAW usa Content-Type audio/basic (formato telef√≥nico nativo)
      res.setHeader('Content-Type', 'audio/basic');
      res.setHeader('Content-Length', audioData.audio.length);
      res.setHeader('Cache-Control', 'public, max-age=3600');
      res.setHeader('X-Audio-Hash', audioData.hash);
      res.setHeader('X-Audio-Language', language);
      res.setHeader('X-Voice-Name', VOICE_NAME);
      res.setHeader('X-Model-Name', MODEL_NAME);
      res.setHeader('X-Vertex-AI', 'true');

      return res.status(200).send(audioData.audio);
    } catch (error) {
      console.error('‚ùå [TTS] Error en POST endpoint:', error);
      return res.status(500).json({ 
        error: 'Error generating audio',
        message: error.message 
      });
    }
  }
  
  return res.status(405).json({ error: 'Method not allowed' });
};
